---
title: 'Correlations'
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: ["xaringan-themer.css", "my-theme.css"]
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

<style type="text/css">
.remark-slide-content {
    font-size: 30px;
    padding: 1em 4em 1em 4em;
}

.small .remark-code { 
  font-size: 80% !important;
}
.tiny .remark-code {
  font-size: 50% !important;
}

</style>


```{r xaringan-themer, include=FALSE, warning=FALSE}
library(xaringanthemer)
style_mono_accent(base_color = "#17139C",link_color = "#DD3E3E", base_font_size = "18px")

```

```{r, echo = F, warning = F, message = F}
library(psych)
library(tidyverse)
library(MASS)
library(ggExtra)
library(RColorBrewer)
purples = brewer.pal(n = 3, name = "Purples")
```

## Last Time

Correlations Galore!

- Equations/calculations
- Uses
- How they can fool you

--

## This Time

Correlations Galore!
- NHST with correlations
- (if time, R Projects & sync to GitHub)

---
### Covariation 

"Sum of the cross-products"


### Population
$$SP_{XY} =\Sigma(X_i−\mu_X)(Y_i−\mu_Y)$$

### Sample
$$ SP_{XY} =\Sigma(X_i−\bar{X})(Y_i−\bar{Y})$$


???

**What does a large, positive SP indicate?**
A positive relationship, same sign

**What does a large, negatve SP indicate?**
A negative relationship, different sign

**What does SP close to 0 indicate?**
No relationship



---

## Covariance
Sort of like the variance of two variables

### Population
$$\sigma_{XY} =\frac{\Sigma(X_i−\mu_X)(Y_i−\mu_Y)}{N}$$

### Sample
$$s_{XY} = cov_{XY} =\frac{\Sigma(X_i−\bar{X})(Y_i−\bar{Y})}{N-1}$$


---

## Covariance table



$$\mathbf{K_{XX}} = \left[\begin{array}
{rrr}
\sigma^2_X & cov_{XY} & cov_{XZ} \\
cov_{YX} & \sigma^2_Y & cov_{YZ} \\
cov_{ZX} & cov_{ZY} & \sigma^2_Z
\end{array}\right]$$


$cov_{xy} = cov_{yx}$ 

---
## Correlation

Pearson product moment correlation

### Population
$$\rho_{XY} = \frac{\Sigma z_Xz_Y}{N} = \frac{SP}{\sqrt{SS_X}\sqrt{SS_Y}} = \frac{\sigma_{XY}}{\sigma_X \sigma_Y}$$
### Sample
$$r_{XY} = \frac{\Sigma z_Xz_Y}{n-1} = \frac{SP}{\sqrt{SS_X}\sqrt{SS_Y}} = \frac{s_{XY}}{s_X s_Y}$$
???

**Why is it called the Pearson Product Moment correlation?**
Pearson = Karl Pearson
Product = multiply
Moment = variance is the second moment of a distribution

---

## Statistical test

Hypothesis testing

$$\large H_{0}: \rho_{xy} = 0$$

$$\large H_{A}: \rho_{xy} \neq 0$$

Assumes:
- Observations are independent
- Symmetric bivariate distribution (joint probability distribution)

---

### Population

```{r, echo=FALSE,cache=TRUE}
mu1<-0 # setting the expected value of x1
mu2<-0 # setting the expected value of x2
s11<-4 # setting the variance of x1
s12<-2 # setting the covariance between x1 and x2
s22<-5 # setting the variance of x2
rho<-0.1 # setting the correlation coefficient between x1 and x2
x1<-seq(-10,10,length=41) # generating the vector series x1
x2<-x1 # copying x1 to x2
f<-function(x1,x2)
{
term1<-1/(2*pi*sqrt(s11*s22*(1-rho^2)))
term2<--1/(2*(1-rho^2))
term3<-(x1-mu1)^2/s11
term4<-(x2-mu2)^2/s22
term5<--2*rho*((x1-mu1)*(x2-mu2))/(sqrt(s11)*sqrt(s22))
term1*exp(term2*(term3+term4-term5))
} # setting up the function of the multivariate normal density
#
z<-outer(x1,x2,f) # calculating the density values
#
persp(x1, x2, z,
main="Joint Probability Distribution", sub=expression(italic(f)~(bold(x))==frac(1,2~pi~sqrt(sigma[11]~ sigma[22]~(1-rho^2)))~phantom(0)^bold(.)~exp~bgroup("{", list(-frac(1,2(1-rho^2)),
bgroup("[", frac((x[1]~-~mu[1])^2, sigma[11])~-~2~rho~frac(x[1]~-~mu[1], sqrt(sigma[11]))~ frac(x[2]~-~mu[2],sqrt(sigma[22]))~+~ frac((x[2]~-~mu[2])^2, sigma[22]),"]")),"}")),
col= purples[3],
theta=30, phi=20,
r=50,
d=0.1,
expand=0.5,
ltheta=90, lphi=180,
shade=0.75,
ticktype="detailed",
nticks=5)

# produces the 3-D plot
mtext(expression(list(mu[1]==0,mu[2]==0,sigma[11]==4,sigma[22]==5,sigma[12 ]==2,rho==0.1)), side=3)
# adding a text line to the graph
```

---

### Sample

```{r, echo = F, warning=FALSE, message=FALSE}
sigma <- matrix( c( 4, 2,
                    2, 5 ), 2, 2 )  # covariance matrix

sample = BDgraph::rmvnorm(n = 30, mean = c(0,0), sigma = sigma)

sample %>% as.data.frame() %>%
  ggplot(aes(x = V1, y = V2)) +
  geom_smooth(method = "lm", se = F, color = purples[2])+
  geom_point(color = purples[3]) +
  scale_x_continuous("X1") + 
  scale_y_continuous("X2")+
  theme_bw(base_size = 20)
```

---

## Sampling distribution?

The sampling distribution we use depends on our null hypothesis.

--

If our null hypothesis is that $(\rho = 0)$ , then we can use a ***t*-distribution** to estimate the statistical significance of a correlation. 



---
## Test Statistic
Signal divided by noise

$$\large t = {\frac{r}{SE_{r}}}$$
--
.pull-left[
$$\large SE_r = \sqrt{\frac{1-r^2}{N-2}}$$

$$\large t = {\frac{r}{\sqrt{\frac{1-r^{2}}{N-2}}}}$$
]

.pull-right[
$$\large DF = N-2$$
]

---
## Power calculations
What sample size do you need in order to have enough power to detect a **.1** correlation? 
```{r, warning = FALSE, message = FALSE}
library(pwr)
pwr.r.test(n = , r = .1, sig.level = .05 , power = .8)
```

---
## Power calculations
What sample size do you need in order to have enough power to detect a **.3** correlation? 
```{r, warning = FALSE, message = FALSE}
library(pwr)
pwr.r.test(n = , r = .3, sig.level = .05 , power = .8)
```
---
## Power calculations
- But what is your confidence?

- N = 84 gives you CI[.09, .48]

- Schönbrodt & Perugini (2013) suggest correlations 'stabilize' at 250+ regardless of effect size



---
## Fisher's r to z' transformation
.left-column[
If we want to make calculations based on $\rho \neq 0$ then we will run into a skewed sampling distribution.
]

```{r, echo = F}
r_sampling = function(x, r, n){
  z = fisherz(r)
  se = 1/(sqrt(n-3))
  x_z = fisherz(x)
  density = dnorm(x_z, mean = z, sd = se)
  return(density)
}


cor_75 = ggplot(data.frame(x = seq(-.99, .99)), aes(x)) +
  stat_function(fun = function(x) r_sampling(x, r = .75, 
                                             n = 30),
                geom = "area", fill = purples[3]) +
  scale_x_continuous(limits = c(-.99, .99)) +
  labs(subtitle = "r = .75, n = 30") +
  theme_bw(base_size = 20) +
  theme(plot.subtitle = element_text(size = 12))

cor_32 = ggplot(data.frame(x = seq(-.99, .99)), aes(x)) +
  stat_function(fun = function(x) r_sampling(x, r = .32, 
                                             n = 30),
                geom = "area", fill = purples[2]) +
  scale_x_continuous(limits = c(-.99, .99)) +
  labs(subtitle = "r = .32, n = 30")+
  theme_bw(base_size = 20) +
  theme(plot.subtitle = element_text(size = 12))


cor_n85 = ggplot(data.frame(x = seq(-.99, .99)), aes(x)) +
  stat_function(fun = function(x) r_sampling(x, r = -.85, 
                                             n = 30),
                geom = "area", fill = purples[3]) +
  scale_x_continuous(limits = c(-.99, .99)) +
  labs(subtitle = "r = -.85, n = 30")+
  theme_bw(base_size = 20) +
  theme(plot.subtitle = element_text(size = 12))

cor_75b = ggplot(data.frame(x = seq(-.99, .99)), aes(x)) +
  stat_function(fun = function(x) r_sampling(x, r = .75, 
                                             n = 150),
                geom = "area", fill = purples[3]) +
  scale_x_continuous(limits = c(-.99, .99)) +
  labs(subtitle = "r = .75, n = 150") +
  theme_bw(base_size = 20) +
  theme(plot.subtitle = element_text(size = 12))

cor_32b = ggplot(data.frame(x = seq(-.99, .99)), aes(x)) +
  stat_function(fun = function(x) r_sampling(x, r = .32, 
                                             n = 150),
                geom = "area", fill = purples[2]) +
  scale_x_continuous(limits = c(-.99, .99)) +
  labs(subtitle = "r = .32, n = 150")+
  theme_bw(base_size = 20) +
  theme(plot.subtitle = element_text(size = 12))


cor_n85b = ggplot(data.frame(x = seq(-.99, .99)), aes(x)) +
  stat_function(fun = function(x) r_sampling(x, r = -.85, 
                                             n = 150),
                geom = "area", fill = purples[3]) +
  scale_x_continuous(limits = c(-.99, .99)) +
  labs(subtitle = "r = -.85, n = 150")+
  theme_bw(base_size = 20) +
  theme(plot.subtitle = element_text(size = 12))


ggpubr::ggarrange(cor_n85, cor_32, cor_75, 
                  cor_n85b, cor_32b, cor_75b)
```


---
## Fisher's r to z' transformation

- Skewed sampling distribution will rear its head when:

    * $H_{0}: \rho \neq 0$
    * Calculating confidence intervals
    * Testing two correlations against one another

- r to z':

$$\large z^{'} = {\frac{1}{2}}ln{\frac{1+r}{1-r}}$$

???
ln = natural log
---
## Fisher’s r to z’ transformation

```{r, echo = F, fig.align='center', out.width="65%"}
r = seq(-.99,.99,.01)
z = psych::fisherz(r)
data.frame(r,z) %>%
  ggplot(aes(x = r, y = z)) +
  geom_line() +
  scale_x_continuous(expr(r))+
  scale_y_continuous(expr(z_r))+
  theme_bw(base_size = 20)
```


No longer bounded by 1 & -1

---
##  Computing confidence interval

1. Transform $r$ into $z'$
2. Compute CI as you normally would using $z'$
3. Revert back to $r$

$$ SE_z = \frac{1}{\sqrt{N-3}}$$

$$\large r = {\frac{e^{2z'}-1}{e^{2z'}+1}}$$

.small[*Note, `e` here stands for Euler's number. `exp(1)` is straight Euler's number or `e`, `exp(2)` is Euler's number squared or `e^2`*]
---

In a sample of 42 students, you calculate a correlation of 0.44 between hours spent outside on Saturday and self-rated health. What is the precision of your estimate?

```{r, echo = F}
r = .44
N = 42
z = fisherz(r)
se = 1/(sqrt(N-3))
ct = qt(p = .975, df = N-2, lower.tail = T)
lbz = z-(se*ct)
ubz = z+(se*ct)
lb = fisherz2r(lbz)
ub = fisherz2r(ubz)
```

$$z^{'} = {\frac{1}{2}}ln{\frac{1+.44}{1-.44}} = `r round(z,2)`$$
$$SE_z = \frac{1}{\sqrt{42-3}} = `r round(se,2)`$$

$$CI_{Z_{LB}} = `r round(z,2)`-(`r round(ct,3)`)`r round(se,2)` = `r round(lbz,2)`$$

$$CI_{Z_{UB}} = `r round(z,2)`+(`r round(ct,3)`)`r round(se,2)` = `r round(ubz,2)`$$



.tiny[Why is it 2.021 instead of 1.96? Because we're using a *t* distribution with N-2 degrees of freedom. `qt(p = .975, df = N-2, lower.tail = T)` = `r round(qt(p = .975, df = N-2, lower.tail = T), digits = 3)`]

---


$$CI_{r_{LB}} = {\frac{e^{2(`r round(lbz,2)`)}-1}{e^{2(`r round(lbz,2)`)}+1}} = `r round(lb,2)`$$

$$CI_{r_{UB}} = = {\frac{e^{2(`r round(ubz,2)`)}-1}{e^{2(`r round(ubz,2)`)}+1}} = `r round(ub,2)`$$

---
## How to do in R

```{r, eval=FALSE}
library(psych)
fisherz(r)
fisherz2r(z)
```

---

## Two independent group test

- Does the correlation in group 1 differ from the correlation in group 2?

$$ H_0: \rho_1 = \rho_2 $$

$$ H_A: \rho_1 \neq \rho_2 $$

- Normally distributed

$$Z = \frac{z_1^{'}- z_2^{'}}{se_{z_1-z_2}}$$

---
### Comparing two correlations

Again, we use Fisher’s $r$ to $z’$ transformation. Here, we're transforming the correlations into $z'$s, then using the difference between $z$'s to calculate the test statistic. 

$$Z = \frac{z_1^{'}- z_2^{'}}{se_{z_1-z_2}}$$

$$se_{z_1-z_2} = \sqrt{se_{z_1}+se_{z_2}} = \sqrt{\frac{1}{n_1-3}+\frac{1}{n_2-3}}$$

- But probably best to do this test in another framework (e.g., GLM via interaction or SEM)
---

### Example

Replication of Hill et al. (2012) where they found that the correlation between narcissism and happiness was greater for young adults compared to older adults

.pull-left[
### Young adults
$$N = 327$$
$$r = .402$$
]

.pull-right[
### Older adults
$$N = 273$$
$$r = .283$$
]



$$H_0:\rho_1 = \rho_2$$
$$H_1:\rho_1 \neq \rho_2$$

---

```{r, echo = F}
z1 = fisherz(.402)
z2 = fisherz(.283)
n1 = 327
n2 = 273
se = sqrt(1/(n1-3) + 1/(n2-3))

zstat = (z1-z2)/se
```


$$z_1^{'} = {\frac{1}{2}}ln{\frac{1+.402}{1-.402}} = `r round(z1,3)`$$

$$z_2^{'} = {\frac{1}{2}}ln{\frac{1+.283}{1-.283}} = `r round(z2,3)`$$

$$se_{z_1-z_2} = \sqrt{\frac{1}{327-3}+\frac{1}{273-3}} = `r round(se,3)`$$

$$\text{Test statistic} = \frac{z_1^{'}-z_2^{'}}{se_{z_1-z_2}} = \frac{`r round(z1,3)`- `r round(z2,3)`}{`r round(se,3)`} = `r round(zstat,3)`$$

```{r}
pnorm(abs(zstat), lower.tail = F)*2
```

.small[*Note: more examples at end of slides, after "next time"*]

---

## Summary of NHST with Correlations

- If "is _r_ different from the number 0, *t*-test" -- **where have you seen this before?**

--

- Use r to z if:
  - Need a CI
  - "is _r_ different from another number that is not 0"
  - Compare 2 correlations against each other

---
## Correlation matrices

Correlations are both a descriptive and an inferential statistic. As a descriptive statistic, they're useful for understanding what's going on in a larger dataset. 

Like we use the `summary()` or `describe()` (psych) functions to examine our dataset _before we run any infernetial tests_, we should also look at the correlation matrix. 

---

```{r}
library(psych)
data(bfi)
head(bfi)
```

---

```{r}
cor(bfi)
```

---

```{r}
round(cor(bfi, use = "pairwise"),2)
```

---

```{r}
round(cor(bfi, use = "complete"),2)
```

---

With **pairwise deletion**, different sets of cases contribute to different correlations.  That maximizes the sample sizes, but can lead to problems if the data are missing for some systematic reason.

**Listwise deletion** ("complete cases") doesn't have the same issue of biasing correlations, but does result in smaller samples and potentially limited generalizability.

A good practice is comparing the different matrices; if the correlation values are very different, this suggests that the missingness that affects pairwise deletion is systematic.

---

```{r}
round(cor(bfi, use = "pairwise")- cor(bfi, use = "complete"),2)
```
---

## Visualizing correlations

For a single correlation, best practice is to visualize the relationship using a scatterplot. A best fit line is advised, as it can help clarify the strength and direction of the relationship. 

[http://guessthecorrelation.com/](http://guessthecorrelation.com/)


See also: [Interpreting Correlations](https://rpsychologist.com/correlation/)
---

```{r, echo = F, warning = FALSE, message=FALSE}
library(datasauRus)
datasaurus_dozen %>%
  filter(dataset == "away") %>%
  ggplot(aes(x = x, y = y)) +
  geom_point()+
  ggtitle(expression(paste(M[X], "= 54.3 ", S[X], "= 16.8 ", 
                           M[Y], "= 47.8 ", S[Y], "= 26.9 ",
                           "R = -.06"))) +
  theme_bw(base_size = 15)
```


---

```{r, echo = F}
datasaurus_dozen %>%
  filter(dataset == "h_lines") %>%
  ggplot(aes(x = x, y = y)) +
  geom_point()+
  ggtitle(expression(paste(M[X], "= 54.3 ", S[X], "= 16.8 ", 
                           M[Y], "= 47.8 ", S[Y], "= 26.9 ",
                           "R = -.06"))) +
  theme_bw(base_size = 15)
```

---

```{r, echo = F}
datasaurus_dozen %>%
  filter(dataset == "x_shape") %>%
  ggplot(aes(x = x, y = y)) +
  geom_point()+
  ggtitle(expression(paste(M[X], "= 54.3 ", S[X], "= 16.8 ", 
                           M[Y], "= 47.8 ", S[Y], "= 26.9 ",
                           "R = -.06"))) +
  theme_bw(base_size = 15)
```

---

```{r, echo = F}
datasaurus_dozen %>%
  filter(dataset == "circle") %>%
  ggplot(aes(x = x, y = y)) +
  geom_point()+
  ggtitle(expression(paste(M[X], "= 54.3 ", S[X], "= 16.8 ", 
                           M[Y], "= 47.8 ", S[Y], "= 26.9 ",
                           "R = -.06"))) +
  theme_bw(base_size = 15)
```


---
```{r, echo = F}
datasaurus_dozen %>%
  filter(dataset == "wide_lines") %>%
  ggplot(aes(x = x, y = y)) +
  geom_point()+
  ggtitle(expression(paste(M[X], "= 54.3 ", S[X], "= 16.8 ", 
                           M[Y], "= 47.8 ", S[Y], "= 26.9 ",
                           "R = -.06"))) +
  theme_bw(base_size = 15)
```


---
```{r, echo = F}
datasaurus_dozen %>%
  filter(dataset == "bullseye") %>%
  ggplot(aes(x = x, y = y)) +
  geom_point()+
  ggtitle(expression(paste(M[X], "= 54.3 ", S[X], "= 16.8 ", 
                           M[Y], "= 47.8 ", S[Y], "= 26.9 ",
                           "R = -.06"))) +
  theme_bw(base_size = 15)
```

---
```{r, echo = F}
datasaurus_dozen %>%
  filter(dataset == "star") %>%
  ggplot(aes(x = x, y = y)) +
  geom_point(size = 2)+
  ggtitle(expression(paste(M[X], "= 54.3 ", S[X], "= 16.8 ", 
                           M[Y], "= 47.8 ", S[Y], "= 26.9 ",
                           "R = -.06"))) +
  theme_bw(base_size = 15)
```

---
```{r, echo = F}
datasaurus_dozen %>%
  filter(dataset == "dino") %>%
  ggplot(aes(x = x, y = y)) +
  geom_point(size = 2)+
  ggtitle(expression(paste(M[X], "= 54.3 ", S[X], "= 16.8 ", 
                           M[Y], "= 47.8 ", S[Y], "= 26.9 ",
                           "R = -.06"))) +
  theme_bw(base_size = 15)
```

---

## Visualizing correlation matrices

A single correlation can be informative; a correlation matrix is more than the sum of its parts. 

Correlation matrices can be used to infer larger patterns of relationships. You may be one of the gifted who can look at a matrix of numbers and see those patterns immediately. Or you can use **heat maps** to visualize correlation matrices. 

```{r, results = 'hide', message=FALSE, warning=FALSE}
library(corrplot)
```

---

```{r}
corrplot(cor(bfi, use = "pairwise"), method = "square")
```

---

![](images/comm plot-1.png)

.small[
[Beck, Condon, & Jackson, 2019](https://psyarxiv.com/857ev/)
]
---
## Other correlation tests:
1. Set of correlations
2. Dependent correlations (i.e., within same group). These are more easily tested via Structural Equation Modeling (SEM)
3. Intra Class Correlation (ICC)

- Again, best to do these tests in another framework (e.g., interaction, SEM, MLM)



---
### Types of correlations

- Many ways to get at relationship between two variables

- Statistically the different types are almost exactly the same

- Exist for historical reasons

---

### Types of correlations
1. Point Biserial
    +  continuous and dichtomous
2. Phi coefficient
    + both dichotomous
3. Spearman & Kendall rank order
    + ranked data (nonparametric)
    + Spearman for larger samples, Kendall for smaller samples (or a lot of ties in rank ordering)
4. Biserial (assumes dichotomous is continuous)
5. Tetrachoric (assumes dichotomous is continuous)
    + both dichotomous
6. Polychoric (assumes continuous)
    + ordinal

---
#### Do the special cases matter?

For Spearman, you'll get a different answer. 
```{r}
x = rnorm(n = 10); y = rnorm(n = 10) #randomly generate 10 numbers from normal distribution
```

.pull-left[
```{r, echo = T}
head(cbind(x,y))
cor(x,y, method = "pearson")
```
]
.pull-right[
```{r}
head(cbind(x,y, rank(x), rank(y)))
cor(x,y, method = "spearman")
```
]

---
#### Do the special cases matter?

If your data are naturally binary, no difference between Pearson and point-biserial.

```{r}
x = rnorm(n = 10); y = rbinom(n = 10, size = 1, prob = .3)
head(cbind(x,y))
```

.pull-left[
```{r, echo = T}
cor(x,y, method = "pearson")
```
]
.pull-right[
```{r}
ltm::biserial.cor(x,y, level = 2)
```
]
---
#### Do the special cases matter?

.tiny[If your data are artificially binary, there can be big differences. DON'T USE MEDIAN SPLITS!]

```{r}
x = rnorm(n = 10); y = rnorm(n = 10)
```
.pull-left[
```{r, echo = T}
head(cbind(x,y))
cor(x,y, method = "pearson")
```
]
.pull-right[
```{r}
d_y = ifelse(y < median(y), 0, 1)
head(cbind(x,y, d_y))
ltm::biserial.cor(x,d_y, level = 2)
```
]

---

class: inverse

## Next time....

Partial and Semi-partial correlations

(remaining slides include more examples, if you want some practice)

---
### Example

The correlation between midterm exam grades and final exam grades was .56. The class size was 104. Is this statistically significant?

```{r, echo = F}
N = 104
r = .56

se = sqrt((1-r^2)/(N-2))

p1 = pt(r/se, df = N-2, lower.tail = F)
```


--
### Using t-method

$$\large SE_r = \sqrt{\frac{1-r^2}{N-2}} = \sqrt{\frac{1-.56^2}{104-2}} = `r round(se,2)`$$
$$\large t = \frac{r}{SE_r} = \frac{`r round(r,2)`}{`r round(se,2)`} = `r round(r/se,2)`$$

---


.pull-left[
Probability of getting a *t* statistic of `r round(r/se,2)` or greater is $3.19 x 10^{-10}$


Probability of getting *t* statistic of `r round(r/se,2)` or more extreme is $6.38 x 10^{-10}$
]

.pull-right[

```{r, echo = F, message = F, warning = F, results='hide'}
library(tidyverse)
data.frame(x = c(-3,7)) %>%
  ggplot(aes(x = x)) +
  stat_function(fun = function(x) dt(x = x, df = N-2), geom = "line") +
  geom_vline(aes(xintercept = r/se), color = "purple") + 
  ggtitle("t distribution (DF = 102)")+
  theme_bw()
```
]
---

### Example

The correlation between midterm exam grades and final exam grades was .56. The class size was 104. Is this statistically significantly different from .40?

--

```{r, echo = F}
r = .56
N = 104
null = .40
zr = psych::fisherz(r)
znull = psych::fisherz(null)
se = 1/sqrt(N-3)
stat = (zr-znull)/se
```

$$\large z^{'} = {\frac{1}{2}}ln{\frac{1+r}{1-r}}= {\frac{1}{2}}ln{\frac{1+`r r`}{1-`r r`}} = `r round(zr,2)`$$
$$\large z^{'}_{H_0} = {\frac{1}{2}}ln{\frac{1+r}{1-r}}= {\frac{1}{2}}ln{\frac{1+`r null`}{1-`r null`}} = `r round(znull,2)`$$
$$ SE_z = \frac{1}{\sqrt{`r N`-3}} = `r round(se,2)`$$

---

$$Z_{\text{statistic}} = \frac{z'-\mu}{SE_z}=\frac{`r round(zr,2)`-`r round(znull,2)`}{`r round(se,2)`} = `r round(stat,2)`$$

```{r}
stat
pnorm(stat, lower.tail = F)
pnorm(stat, lower.tail = F)*2
```

pagedown::chrome_print("5-correlation.html")